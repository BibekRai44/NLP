{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import pandas library\n\nimport pandas as pd\n\n#read the dataset \"news_dataset.json\" provided and load it into dataframe \"df\"\n\ndf=pd.read_json('/kaggle/input/news-json-datafile/news_dataset.json')\n\n\n#print the shape of data\n\ndf.shape\n\n#print the top5 rows\n\ndf.head(5)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-12T07:47:36.928654Z","iopub.execute_input":"2023-09-12T07:47:36.929490Z","iopub.status.idle":"2023-09-12T07:47:37.468203Z","shell.execute_reply.started":"2023-09-12T07:47:36.929443Z","shell.execute_reply":"2023-09-12T07:47:37.466888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the distribution of labels \n\ndf.category.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:48:08.162643Z","iopub.execute_input":"2023-09-12T07:48:08.163168Z","iopub.status.idle":"2023-09-12T07:48:08.182982Z","shell.execute_reply.started":"2023-09-12T07:48:08.163124Z","shell.execute_reply":"2023-09-12T07:48:08.182016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Add the new column \"label_num\" which gives a unique number to each of these labels \n\ndf['label_num']=df['category'].map({'CRIME':1,'SPORTS':2,'BUSINESS':3})\n\n#check the results with top 5 rows\n\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:49:40.206505Z","iopub.execute_input":"2023-09-12T07:49:40.206907Z","iopub.status.idle":"2023-09-12T07:49:40.227862Z","shell.execute_reply.started":"2023-09-12T07:49:40.206875Z","shell.execute_reply":"2023-09-12T07:49:40.226636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use this utility function to preprocess the text\n#1. Remove the stop words\n#2. Convert to base form using lemmatisation\nimport spacy\nnlp=spacy.load(\"en_core_web_lg\")\ndef preprocess(text):\n    doc = nlp(text)\n    filtered_tokens = []\n    for token in doc:\n        if token.is_stop or token.is_punct:\n            continue\n        filtered_tokens.append(token.lemma_)\n    return ' '.join(filtered_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:56:57.587382Z","iopub.execute_input":"2023-09-12T07:56:57.588612Z","iopub.status.idle":"2023-09-12T07:57:03.822960Z","shell.execute_reply.started":"2023-09-12T07:56:57.588564Z","shell.execute_reply":"2023-09-12T07:57:03.821805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a new column \"preprocessed_text\" which store the clean form of given text [use apply and lambda function]\n\ndf['preprocessed_text']=df['text'].apply(lambda text: preprocess(text))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:57:07.719069Z","iopub.execute_input":"2023-09-12T07:57:07.719483Z","iopub.status.idle":"2023-09-12T07:58:46.158567Z","shell.execute_reply.started":"2023-09-12T07:57:07.719452Z","shell.execute_reply":"2023-09-12T07:58:46.157244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#print the top 5 rows\n\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:58:57.753665Z","iopub.execute_input":"2023-09-12T07:58:57.754041Z","iopub.status.idle":"2023-09-12T07:58:57.766883Z","shell.execute_reply.started":"2023-09-12T07:58:57.754013Z","shell.execute_reply":"2023-09-12T07:58:57.765652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a new column \"vector\" that store the vector representation of each pre-processed text\n\ndf['vector']=df['text'].apply(lambda text:nlp(text).vector)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:14:25.660954Z","iopub.execute_input":"2023-09-12T08:14:25.661395Z","iopub.status.idle":"2023-09-12T08:16:01.455815Z","shell.execute_reply.started":"2023-09-12T08:14:25.661356Z","shell.execute_reply":"2023-09-12T08:16:01.454205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the top 5 rows\n\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:16:55.133661Z","iopub.execute_input":"2023-09-12T08:16:55.134041Z","iopub.status.idle":"2023-09-12T08:16:55.156754Z","shell.execute_reply.started":"2023-09-12T08:16:55.134013Z","shell.execute_reply":"2023-09-12T08:16:55.155389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=df['vector'].values\ny=df['label_num']","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:26:32.948867Z","iopub.execute_input":"2023-09-12T08:26:32.949246Z","iopub.status.idle":"2023-09-12T08:26:32.954762Z","shell.execute_reply.started":"2023-09-12T08:26:32.949216Z","shell.execute_reply":"2023-09-12T08:26:32.953431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(x,y,random_state=42,test_size=0.2,stratify=df['label_num'])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:26:34.257598Z","iopub.execute_input":"2023-09-12T08:26:34.257988Z","iopub.status.idle":"2023-09-12T08:26:34.271901Z","shell.execute_reply.started":"2023-09-12T08:26:34.257960Z","shell.execute_reply":"2023-09-12T08:26:34.270636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n\nimport numpy as np\n\n\n#reshapes the X_train and X_test using 'stack' function of numpy. Store the result in new variables \"X_train_2d\" and \"X_test_2d\"\n\nX_train_2d=np.stack(X_train)\nX_test_2d=np.stack(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:26:35.443970Z","iopub.execute_input":"2023-09-12T08:26:35.444374Z","iopub.status.idle":"2023-09-12T08:26:35.474873Z","shell.execute_reply.started":"2023-09-12T08:26:35.444344Z","shell.execute_reply":"2023-09-12T08:26:35.473647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Attempt 1:\n\nuse spacy glove embeddings for text vectorization.\n\nuse Decision Tree as the classifier.\n\nprint the classification report.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.metrics import classification_report\n#1. creating a Decision Tree model object\n\nmodel=DecisionTreeClassifier()\n\n\n#2. fit with all_train_embeddings and y_train\n\nmodel.fit(X_train_2d,y_train)\n\n\n#3. get the predictions for all_test_embeddings and store it in y_pred\n\ny_pred=model.predict(X_test_2d)\n\n#4. print the classfication report\n\nprint(classification_report(y_test,y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:26:38.193788Z","iopub.execute_input":"2023-09-12T08:26:38.194633Z","iopub.status.idle":"2023-09-12T08:26:41.160857Z","shell.execute_reply.started":"2023-09-12T08:26:38.194586Z","shell.execute_reply":"2023-09-12T08:26:41.159610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Attempt 2:\n\nuse spacy glove embeddings for text vectorization.\nuse MultinomialNB as the classifier after applying the MinMaxscaler.\nprint the classification report.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import MinMaxScaler\nmodel=MinMaxScaler()\nnew_train_data=model.fit_transform(X_train_2d)\nnew_test_data=model.transform(X_test_2d)\nclf=MultinomialNB()\nclf.fit(new_train_data,y_train)\nclf.predict(X_test_2d)\nprint(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:58:36.415146Z","iopub.execute_input":"2023-09-12T08:58:36.415607Z","iopub.status.idle":"2023-09-12T08:58:36.484842Z","shell.execute_reply.started":"2023-09-12T08:58:36.415539Z","shell.execute_reply":"2023-09-12T08:58:36.481121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Attempt 3:\n\nuse spacy glove embeddings for text vectorization.\nuse KNeighborsClassifier as the classifier after applying the MinMaxscaler.\nprint the classification report.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nmodel=KNeighborsClassifier(n_neighbors=5,metric='euclidean')\nmodel.fit(new_train_data,y_train)\nmodel.predict(X_test_2d)\nprint(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T09:08:49.821229Z","iopub.execute_input":"2023-09-12T09:08:49.821647Z","iopub.status.idle":"2023-09-12T09:08:50.160748Z","shell.execute_reply.started":"2023-09-12T09:08:49.821617Z","shell.execute_reply":"2023-09-12T09:08:50.159627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Attempt 4:\n\nuse spacy glove embeddings for text vectorization.\nuse RandomForestClassifier as the classifier after applying the MinMaxscaler.\nprint the classification report.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier()\nmodel.fit(new_train_data,y_train)\nmodel.predict(X_test_2d)\nprint(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T09:09:44.930558Z","iopub.execute_input":"2023-09-12T09:09:44.930942Z","iopub.status.idle":"2023-09-12T09:09:55.641578Z","shell.execute_reply.started":"2023-09-12T09:09:44.930912Z","shell.execute_reply":"2023-09-12T09:09:55.640404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Attempt 5:\n\nuse spacy glove embeddings for text vectorization.\nuse GradientBoostingClassifier as the classifier after applying the MinMaxscaler.\nprint the classification report.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nmodel=GradientBoostingClassifier()\nmodel.fit(new_train_data,y_train)\nmodel.predict(X_test_2d)\nprint(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T09:10:51.822499Z","iopub.execute_input":"2023-09-12T09:10:51.823339Z","iopub.status.idle":"2023-09-12T09:14:27.804207Z","shell.execute_reply.started":"2023-09-12T09:10:51.823298Z","shell.execute_reply":"2023-09-12T09:14:27.802636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.figure(figsize=(5,5))\nsns.heatmap(cm,annot=True,fmt='d')\nplt.xlabel(\"prediction\")\nplt.ylabel(\"truth\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T09:16:21.550035Z","iopub.execute_input":"2023-09-12T09:16:21.550452Z","iopub.status.idle":"2023-09-12T09:16:22.323792Z","shell.execute_reply.started":"2023-09-12T09:16:21.550420Z","shell.execute_reply":"2023-09-12T09:16:22.321883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}